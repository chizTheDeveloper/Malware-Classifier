# -*- coding: utf-8 -*-
"""Malware_V3_Final_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WlLork1MQDf1Q8EuMYQlsYv622jIaBYQ
"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix
from sklearn.svm import SVC
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder

"""**Load Data**"""

#read csv
df = pd.read_csv('/content/drive/MyDrive/Malware Dataset/sampled_data.csv')

"""**Data exploration**"""

df.shape

df.head()

# Assuming df is your DataFrame
pd.set_option('display.max_rows', None)

# Check for missing values
missing_values = df.isnull().sum()

# Calculate the percentage of missing values
total_rows = len(df)
missing_percentage = (missing_values / total_rows) * 100

# Convert missing percentage to string format with '%'
missing_percentage_str = missing_percentage.apply(lambda x: f"{x:.0f}%" if not pd.isnull(x) else "")

print("Percentage of Missing Values:")
print(missing_percentage_str.head(76))

df.describe()

pd.set_option('display.max_rows', None)
# Check for missing values
missing_values = df.isnull().sum()
print("Missing Values:")
missing_values.head(76)

# Check for duplicate rows
duplicate_rows = df.duplicated().sum()
print("\nDuplicate Rows:")
print(duplicate_rows)

# Check for categorical columns with too many unique values
categorical_columns = df.select_dtypes(include=['object']).columns
for col in categorical_columns:
    unique_values_count = df[col].nunique()
    if unique_values_count > 10:  # Adjust the threshold as needed
        print(f"\nvalues: {col} has {unique_values_count} unique values")

# Check for categorical columns with NaN values
categorical_nan_columns = {}
for col in categorical_columns:
    nan_count = df[col].isnull().sum()
    if nan_count > 0:
        categorical_nan_columns[col] = nan_count

print("\nCategorical Columns with NaN Values and their Counts:")
for col, count in categorical_nan_columns.items():
    print(f"{col}: {count}")

df.drop(['MachineIdentifier'], axis=1, inplace=True)

df['HasDetections'].value_counts()

"""**Graphs**"""

#  plot_feature
def plot_feature(feature):
    temp = df[df['HasDetections'] == 1][feature].value_counts()
    df[feature].value_counts().plot.bar()
    plt.title(f"Percentage of Detections for {feature}")
    plt.xlabel(feature)
    plt.ylabel("Number of Occurrences")
    plt.xticks(rotation=90)
    plt.show()

plot_feature('Census_IsTouchEnabled')

plot_feature('EngineVersion')

plot_feature('AppVersion')

"""**Data Cleaning**"""

def missing_values_percentage(df):

    # Calculate the total number of missing values for each column
    missing_values = df.isnull().sum()

    # Calculate the total number of entries in the DataFrame
    total_entries = df.shape[0]

    # Calculate the percentage of missing values for each column
    missing_percentage = (missing_values / total_entries) * 100

    return missing_percentage

missing_values_percentage(df)

def remove_columns_missing_data(df, threshold=0.1):

    # Calculate the percentage of missing values for each column
    missing_percentage = (df.isnull().sum() / len(df))

    # Filter out columns with missing value percentages greater than or equal to the threshold
    columns_to_remove = missing_percentage[missing_percentage >= threshold].index

    # Remove the columns from the DataFrame
    df_cleaned = df.drop(columns=columns_to_remove)

    return df_cleaned

cleaned_df = remove_columns_missing_data(df, threshold=0.1)

# Calculate the percentage of each category in each column
category_percentages = cleaned_df.apply(lambda col: col.value_counts(normalize=True).max())

# Check if any column has 90% or more of its features with only one category
columns_to_remove = category_percentages[category_percentages >= 0.9].index

# Remove columns that meet this condition
cleaned_df = cleaned_df.drop(columns=columns_to_remove)

# Print the removed columns
print("Columns removed:")
print(columns_to_remove)

# Display the filtered DataFrame
print("\nFiltered DataFrame:")
cleaned_df.columns

cleaned_df.shape

# Check for missing values
df_filtered = categorized_df.copy()
missing_values_n = df_filtered.isnull().sum()
print("Missing Values:")
missing_values_n.head(76)

# List of categorical columns with NaN values
categorical_columns_with_nan = ['OsBuildLab', 'Census_PrimaryDiskTypeName', 'Census_ChassisTypeName', 'Census_PowerPlatformRoleName']

# Loop through each categorical column and replace NaN values with mode
for col in categorical_columns_with_nan:
    mode_value = df_filtered[col].mode()[0]  # Calculate mode for the column
    df[col].fillna(mode_value, inplace=True)  # Replace NaN values with mode

# List of categorical columns with NaN values
numeric_columns = df_filtered.select_dtypes(include=['int64', 'float64']).columns

# Loop through each numeric column and replace NaN values with median
for col in numeric_columns:
    median_value = df_filtered[col].median()  # Calculate median for the column
    df_filtered[col].fillna(median_value, inplace=True)  # Replace NaN values with median

# Check if there are any remaining NaN values after replacement
remaining_nan_values = df_filtered.isnull().sum().sum()
print("Remaining NaN values after replacement:", remaining_nan_values)

"""**Feature Eng**"""

# Columns to convert to category dtype
columns_to_convert = ['CountryIdentifier', 'SkuEdition', 'Census_ProcessorCoreCount',
                      'Census_OSUILocaleIdentifier']

# Convert columns to category dtype
df_filtered[columns_to_convert] = df_filtered[columns_to_convert].astype('object')

# Engineer the difference
df_filtered['DiskCapacityDifference'] = df_filtered['Census_SystemVolumeTotalCapacity'] - df_filtered['Census_PrimaryDiskTotalCapacity']

# Columns to encode separately
columns_separate = ['Census_OSVersion', 'Census_OSBuildRevision', 'Census_InternalBatteryNumberOfCharges', 'AvSigVersion']

# Columns to encode together
columns_together = ['CountryIdentifier', 'Census_InternalBatteryNumberOfCharges']

# Frequency encoding for columns separately
for column in columns_separate:
    freq_encoding = df_filtered[column].value_counts(normalize=True)
    df_filtered[f'{column}_freq_encoded'] = df_filtered[column].map(freq_encoding)

"""**Label Encode** Categorical colums are label encoded"""

label_data = df_filtered.copy()
# Check for non-numeric values in numeric columns and replace with mean
for col in label_data.select_dtypes(include=['float64', 'int64']).columns:
    non_numeric_values_mask = pd.to_numeric(label_data[col], errors='coerce').isnull()
    if non_numeric_values_mask.any():
        mean_value = label_data.loc[~non_numeric_values_mask, col].mean()
        label_data.loc[non_numeric_values_mask, col] = mean_value

cat_columns = []

for col in label_data.columns:
    if label_data[col].dtype == 'object':
        cat_columns.append(col)

        # Create a LabelEncoder object
label_encoder = LabelEncoder()

# Apply label encoding to each categorical column
for col in cat_columns:
    label_data[col] = label_encoder.fit_transform(label_data[col])

label_data.head()

for column in label_data.columns:
    print(f"Column: {column}")
    print(label_data[column].dtype)
    print(label_data[column].value_counts())
    print(label_data[column].nunique())
    print()

# Plot correlation matrix
correlation_matrix =label_data.corr()
plt.figure(figsize=(24, 18))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f",linewidths=0.5, annot_kws={"size": 5})
plt.title('Correlation Matrix')
plt.tight_layout()
plt.show()

# Find features most correlated with 'HasDetection' (target class)
target_correlation = label_data.corr()['HasDetections'].abs().sort_values(ascending=False)
top_correlated_features = target_correlation[1:10].index  # Exclude 'HasDetection' itself

# Select the dataset containing only the top correlated features
correlated_df = label_data[top_correlated_features]

# Plot correlation matrix heatmap for the selected dataset
correlation_matrix = correlated_df.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5, annot_kws={"size": 10})
plt.title('Correlation Matrix of Features Most Correlated with HasDetection')
plt.tight_layout()
plt.show()

#Identify categorical variables
frequency_data = label_data.copy()

for column in frequency_data.columns:
    print(f"Column: {column}")
    print(frequency_data[column].dtype)
    print(frequency_data[column].value_counts())
    print(frequency_data[column].nunique())
    print()

print(df.shape, frequency_data.shape)

"""**Outlier Detection**"""

# Loop through each column in the dataset
for col in frequency_data.select_dtypes(include=['float64', 'int64']).columns:
    # Calculate Z-scores
    z_scores = np.abs((frequency_data[col] - frequency_data[col].mean()) / frequency_data[col].std())

    # Define threshold for outliers detection
    threshold = 3

    # Get indices of outliers
    outlier_indices = np.where(z_scores > threshold)[0]

    # Print column name if outliers are found
    if len(outlier_indices) > 0:
        print(f"Outliers found in column '{col}': {len(outlier_indices)} outliers.")

# Loop through each column in the dataset
for col in frequency_data.select_dtypes(include=['float64', 'int64']).columns:
    # Calculate the median
    median = frequency_data[col].median()

    # Calculate the interquartile range (IQR)
    Q1 = frequency_data[col].quantile(0.25)
    Q3 = frequency_data[col].quantile(0.75)
    IQR = Q3 - Q1

    # Define thresholds for outlier detection
    lower_threshold = Q1 - 1.5 * IQR
    upper_threshold = Q3 + 1.5 * IQR

    # Get indices of outliers
    outlier_indices = (frequency_data[col] < lower_threshold) | (frequency_data[col] > upper_threshold)

    # Print column name and number of outliers found
    if outlier_indices.any():
        print(f"Outliers found in column '{col}': {outlier_indices.sum()} outliers.")

# Define outlier handling function using Winsorization
def winsorize_outliers(series, lower_quantile=0.01, upper_quantile=0.99):
    lower_bound = series.quantile(lower_quantile)
    upper_bound = series.quantile(upper_quantile)
    series = np.where(series < lower_bound, lower_bound, series)
    series = np.where(series > upper_bound, upper_bound, series)
    return series

# Columns with outliers
columns_with_outliers = [
    'AVProductStatesIdentifier', 'AVProductsInstalled', 'OrganizationIdentifier',
    'OsBuild', 'IeVerIdentifier', 'Census_OEMNameIdentifier',
    'Census_OEMModelIdentifier', 'Census_ProcessorCoreCount',
    'Census_ProcessorManufacturerIdentifier', 'Census_ProcessorModelIdentifier',
    'Census_PrimaryDiskTotalCapacity', 'Census_SystemVolumeTotalCapacity',
    'Census_TotalPhysicalRAM', 'Census_InternalPrimaryDiagonalDisplaySizeInInches',
    'Census_InternalPrimaryDisplayResolutionHorizontal', 'Census_InternalPrimaryDisplayResolutionVertical',
    'Census_OSBuildNumber', 'Census_OSBuildRevision', 'Census_OSInstallLanguageIdentifier',
    'Census_IsTouchEnabled', 'AvSigVersion_freq_encode', 'Census_GenuineStateName_freq_encode',
    'Census_ActivationChannel_freq_encode'
]

# Handle outliers and create a new dataset
df_no_outliers = frequency_data.copy()  # Assuming df is your original dataset

for col in columns_with_outliers:
    if col in df_no_outliers.columns:
        df_no_outliers[col] = winsorize_outliers(df_no_outliers[col])

# Verify if outliers are removed
for col in columns_with_outliers:
    if col in df_no_outliers.columns:
        print(f"Outliers found in column '{col}': {len(df_no_outliers[df_no_outliers[col] != winsorize_outliers(df_no_outliers[col])])} outliers.")

df_no_outliers.columns

"""**Feature Selection**"""

# Calculate correlations
correlations = frequency_data.corrwith(frequency_data['HasDetections']).abs().sort_values(ascending=False)

# Select top correlated features (excluding the target variable itself)
top_correlated_features = correlations[1:52]
print("Top correlated features with HasDetections:")
print(top_correlated_features)

top_correlated_feature_names = top_correlated_features.index
dataset_1 = frequency_data[top_correlated_feature_names]
dataset_1['HasDetections'] = frequency_data['HasDetections']
dataset_1.shape

from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.impute import SimpleImputer

# Impute missing values with median
imputer = SimpleImputer(strategy='median')
frequency_data_imputed = imputer.fit_transform(frequency_data)

# Define the number of features to select
k = 20

# Initialize SelectKBest with ANOVA F-value scoring function
selector = SelectKBest(score_func=f_classif, k=k)

# Perform feature selection
X_selected = selector.fit_transform(frequency_data_imputed, frequency_data['HasDetections'])

# Get the indices of the selected features
selected_indices = selector.get_support(indices=True)

# Get the names of the selected features
selected_feature_names = frequency_data.columns[selected_indices]

# Print the selected feature names
print("Selected Features:")
print(selected_feature_names)

dataset_2= frequency_data[selected_feature_names]
dataset_2['HasDetections'] = frequency_data['HasDetections']
dataset_2.shape

dataset_2.dtypes

dataset_2.head()

"""MODEL **TESTING**

**EVALUATION:** frequency_data using top 20 correlated features
"""

X = dataset_2.drop(columns=['HasDetections'])
y = dataset_2['HasDetections']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Split the data into training and testing sets
X = dataset_2.drop(columns=['HasDetections'])
y = dataset_2['HasDetections']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Logistic Regression
logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)
logistic_predictions = logistic_model.predict(X_test)
logistic_accuracy = accuracy_score(y_test, logistic_predictions)
print("Logistic Regression Accuracy:", logistic_accuracy)

# Random Forest
random_forest_model = RandomForestClassifier()
random_forest_model.fit(X_train, y_train)
random_forest_predictions = random_forest_model.predict(X_test)
random_forest_accuracy = accuracy_score(y_test, random_forest_predictions)
print("Random Forest Accuracy:", random_forest_accuracy)

import xgboost as xgb

# Initialize an XGBoost classifier
xgb_classifier = xgb.XGBClassifier()

# Train the classifier on the selected features
xgb_classifier.fit(X_train, y_train)

# Predictions on the test set
y_pred = xgb_classifier.predict(X_test)

# Evaluate the classifier
from sklearn.metrics import accuracy_score, classification_report

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Classification report
print(classification_report(y_test, y_pred))

import lightgbm as lgb


# Define LightGBM parameters
params = {
    'boosting_type': 'gbdt',
    'objective': 'binary',  # Binary classification task
    'metric': 'binary_error',  # Metric to evaluate the model
    'num_leaves': 31,  # Maximum number of leaves in one tree
    'learning_rate': 0.1,  # Learning rate
    'feature_fraction': 0.9,  # Feature fraction for training
    'bagging_fraction': 0.8,  # Bagging fraction for training
    'bagging_freq': 5,  # Frequency for bagging
    'verbose': 0  # Verbosity
}

# Create LightGBM dataset
train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)  # Specify reference dataset for early stopping

# Train LightGBM model with early stopping
num_round = 100  # Number of boosting rounds
bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])

# Evaluate the model
y_pred = bst.predict(X_test)
accuracy = accuracy_score(y_test, y_pred.round())
print("Accuracy:", accuracy)

"""EVALUATION: cleaned data"""

df_no_outliers.shape

from sklearn.preprocessing import MinMaxScaler

# Assuming df_no_outliers is your DataFrame
numerical_columns = df_no_outliers.select_dtypes(include=['int', 'float']).columns

# Create a MinMaxScaler object
scaler = MinMaxScaler()

# Normalize the numerical columns
df_no_outliers[numerical_columns] = scaler.fit_transform(df_no_outliers[numerical_columns])

df_no_outliers.head()

X = df_no_outliers.drop(columns=['HasDetections'])
y = df_no_outliers['HasDetections']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import xgboost as xgb

# Initialize an XGBoost classifier
xgb_classifier = xgb.XGBClassifier()

# Train the classifier on the selected features
xgb_classifier.fit(X_train, y_train)

# Predictions on the test set
y_pred = xgb_classifier.predict(X_test)

# Evaluate the classifier
from sklearn.metrics import accuracy_score, classification_report

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Classification report
print(classification_report(y_test, y_pred))

import lightgbm as lgb


# Define LightGBM parameters
params = {
    'boosting_type': 'gbdt',
    'colsample_bytree':0.5, 'early_stopping_rounds': 100,
    'objective': 'binary',  # Binary classification task
    'metric': 'binary_error',  # Metric to evaluate the model
    'num_leaves': 2048,  # Maximum number of leaves in one tree
    'learning_rate': 0.025,  # Learning rate
    'bagging_fraction': 0.8,  # Bagging fraction for training
    'bagging_freq': 5,  # Frequency for bagging
    'verbose': 0  # Verbosity
}

# Create LightGBM dataset
train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)  # Specify reference dataset for early stopping

# Train LightGBM model with early stopping
num_round = 100  # Number of boosting rounds
bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])

# Evaluate the model
y_pred = bst.predict(X_test)
accuracy = accuracy_score(y_test, y_pred.round())
print("Accuracy:", accuracy)

import joblib

# Save the model to a file
joblib.dump(xgb_classifier, 'xgb_classifier_model.pkl')